{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Activation func\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Reading and Cleaning\n",
    "\n",
    "encoder = {\"NEAR BAY\": 1, \"<1H OCEAN\": 2, \"INLAND\": 3, \"NEAR OCEAN\": 4}\n",
    "\n",
    "dataset = pd.read_csv(\"housing.csv\")\n",
    "\n",
    "dataset[\"total_bedrooms\"] = dataset[\"total_bedrooms\"].fillna(dataset[\"total_bedrooms\"].mean())\n",
    "dataset[\"ocean_proximity\"] = dataset[\"ocean_proximity\"].map(encoder).fillna(0)\n",
    "y = dataset[\"median_house_value\"].values.reshape(-1, 1)\n",
    "dataset.drop(labels=\"median_house_value\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train/Test Split\n",
    "\n",
    "threshold = int(0.7 * len(y))\n",
    "\n",
    "X_train = dataset[:threshold]\n",
    "X_test = dataset[threshold:]\n",
    "\n",
    "y_train = y[:threshold]\n",
    "y_test = y[threshold:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalizing\n",
    "X_mean = np.mean(X_train, axis=0)\n",
    "X_std = np.std(X_train, axis=0)\n",
    "X_train_norm = (X_train - X_mean) / X_std\n",
    "\n",
    "y_mean = np.mean(y_train, axis=0)\n",
    "y_std = np.std(y_train, axis=0)\n",
    "y_train_norm = (y_train - y_mean) / y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.9999906468191553\n",
      "Epoch: 100, Loss: 0.9999848405760453\n",
      "Epoch: 200, Loss: 0.9999794932920532\n",
      "Epoch: 300, Loss: 0.9999717462420796\n",
      "Epoch: 400, Loss: 0.9999600891571233\n",
      "Epoch: 500, Loss: 0.9999416614651607\n",
      "Epoch: 600, Loss: 0.9999106522400087\n",
      "Epoch: 700, Loss: 0.9998541253830129\n",
      "Epoch: 800, Loss: 0.9997394711612647\n",
      "Epoch: 900, Loss: 0.9994677815508167\n",
      "Epoch: 1000, Loss: 0.9986461508986999\n",
      "Epoch: 1100, Loss: 0.9947703937286227\n",
      "Epoch: 1200, Loss: 0.9415471169610895\n",
      "Epoch: 1300, Loss: 0.4541867672229473\n",
      "Epoch: 1400, Loss: 0.39084743876097905\n",
      "Epoch: 1500, Loss: 0.3595651602106662\n",
      "Epoch: 1600, Loss: 0.3432273816141177\n",
      "Epoch: 1700, Loss: 0.3308757532348527\n",
      "Epoch: 1800, Loss: 0.3200980386751607\n",
      "Epoch: 1900, Loss: 0.31054160946572296\n",
      "Epoch: 2000, Loss: 0.30277890180071926\n",
      "Epoch: 2100, Loss: 0.29682702780708387\n",
      "Epoch: 2200, Loss: 0.29239515249955245\n",
      "Epoch: 2300, Loss: 0.28937242541673924\n",
      "Epoch: 2400, Loss: 0.2872765065058678\n",
      "Epoch: 2500, Loss: 0.2857173097558369\n",
      "Epoch: 2600, Loss: 0.28439838046326693\n",
      "Epoch: 2700, Loss: 0.28314377087499365\n",
      "Epoch: 2800, Loss: 0.2819195532200531\n",
      "Epoch: 2900, Loss: 0.28068674767119994\n",
      "Epoch: 3000, Loss: 0.27950272931521475\n",
      "Epoch: 3100, Loss: 0.2783676411843117\n",
      "Epoch: 3200, Loss: 0.277202752188857\n",
      "Epoch: 3300, Loss: 0.27612401385549723\n",
      "Epoch: 3400, Loss: 0.27512559564640593\n",
      "Epoch: 3500, Loss: 0.2741495310826959\n",
      "Epoch: 3600, Loss: 0.2733238244262786\n",
      "Epoch: 3700, Loss: 0.2726067868098516\n",
      "Epoch: 3800, Loss: 0.27197160703375367\n",
      "Epoch: 3900, Loss: 0.2714304152474639\n",
      "Epoch: 4000, Loss: 0.270939090479768\n",
      "Epoch: 4100, Loss: 0.27047666823480465\n",
      "Epoch: 4200, Loss: 0.2700207932243982\n",
      "Epoch: 4300, Loss: 0.2696327878825066\n",
      "Epoch: 4400, Loss: 0.26927731237670016\n",
      "Epoch: 4500, Loss: 0.2689518085590326\n",
      "Epoch: 4600, Loss: 0.2686649086109579\n",
      "Epoch: 4700, Loss: 0.26841799806247\n",
      "Epoch: 4800, Loss: 0.2682169022073055\n",
      "Epoch: 4900, Loss: 0.26804404044071783\n",
      "Epoch: 5000, Loss: 0.2678943429055208\n",
      "Epoch: 5100, Loss: 0.2677678365365661\n",
      "Epoch: 5200, Loss: 0.2676605505766184\n",
      "Epoch: 5300, Loss: 0.26754874549768554\n",
      "Epoch: 5400, Loss: 0.2674381452607448\n",
      "Epoch: 5500, Loss: 0.2673571991132967\n",
      "Epoch: 5600, Loss: 0.26728996354226886\n",
      "Epoch: 5700, Loss: 0.2672259000337541\n",
      "Epoch: 5800, Loss: 0.26716047812529475\n",
      "Epoch: 5900, Loss: 0.26709750873791266\n",
      "Epoch: 6000, Loss: 0.2670304061470334\n",
      "Epoch: 6100, Loss: 0.26696302498506747\n",
      "Epoch: 6200, Loss: 0.26688936369078436\n",
      "Epoch: 6300, Loss: 0.2667785296753044\n",
      "Epoch: 6400, Loss: 0.26667839137696503\n",
      "Epoch: 6500, Loss: 0.266590003887215\n",
      "Epoch: 6600, Loss: 0.2665142331704374\n",
      "Epoch: 6700, Loss: 0.2664559855007844\n",
      "Epoch: 6800, Loss: 0.2664150146320387\n",
      "Epoch: 6900, Loss: 0.26638187252171874\n",
      "Epoch: 7000, Loss: 0.26634547497438193\n",
      "Epoch: 7100, Loss: 0.2662960533488783\n",
      "Epoch: 7200, Loss: 0.26624003131581664\n",
      "Epoch: 7300, Loss: 0.26618414853481387\n",
      "Epoch: 7400, Loss: 0.2661381499777671\n",
      "Epoch: 7500, Loss: 0.26609856268531396\n",
      "Epoch: 7600, Loss: 0.26606598324000386\n",
      "Epoch: 7700, Loss: 0.2660371044065019\n",
      "Epoch: 7800, Loss: 0.2660128619293296\n",
      "Epoch: 7900, Loss: 0.2659867643425982\n",
      "Epoch: 8000, Loss: 0.2659654322450415\n",
      "Epoch: 8100, Loss: 0.2659475099181681\n",
      "Epoch: 8200, Loss: 0.2659320291390948\n",
      "Epoch: 8300, Loss: 0.2659156349949154\n",
      "Epoch: 8400, Loss: 0.26589731947686024\n",
      "Epoch: 8500, Loss: 0.2658740018563291\n",
      "Epoch: 8600, Loss: 0.265838870385359\n",
      "Epoch: 8700, Loss: 0.2657985329381013\n",
      "Epoch: 8800, Loss: 0.26576157277513396\n",
      "Epoch: 8900, Loss: 0.26573519507491783\n",
      "Epoch: 9000, Loss: 0.2657150077742522\n",
      "Epoch: 9100, Loss: 0.26569569832818063\n",
      "Epoch: 9200, Loss: 0.26566448036961166\n",
      "Epoch: 9300, Loss: 0.2656323579850764\n",
      "Epoch: 9400, Loss: 0.2656002261361667\n",
      "Epoch: 9500, Loss: 0.26557047139212886\n",
      "Epoch: 9600, Loss: 0.26553408284422125\n",
      "Epoch: 9700, Loss: 0.2654919899132608\n",
      "Epoch: 9800, Loss: 0.26545605695156466\n",
      "Epoch: 9900, Loss: 0.2654109031506162\n",
      "Epoch: 10000, Loss: 0.26536673073169986\n",
      "Epoch: 10100, Loss: 0.2653165947629174\n",
      "Epoch: 10200, Loss: 0.26527258583001234\n",
      "Epoch: 10300, Loss: 0.2652250809201273\n",
      "Epoch: 10400, Loss: 0.26511624387741733\n",
      "Epoch: 10500, Loss: 0.26494529396348593\n",
      "Epoch: 10600, Loss: 0.26480915057576876\n",
      "Epoch: 10700, Loss: 0.26455846181104564\n",
      "Epoch: 10800, Loss: 0.26433634247600496\n",
      "Epoch: 10900, Loss: 0.2641528086239986\n",
      "Epoch: 11000, Loss: 0.26398166032657105\n",
      "Epoch: 11100, Loss: 0.2637738616256085\n",
      "Epoch: 11200, Loss: 0.26352961639383743\n",
      "Epoch: 11300, Loss: 0.26328590286785303\n",
      "Epoch: 11400, Loss: 0.263011269352924\n",
      "Epoch: 11500, Loss: 0.2626122400297827\n",
      "Epoch: 11600, Loss: 0.2622499362385329\n",
      "Epoch: 11700, Loss: 0.2619984361908557\n",
      "Epoch: 11800, Loss: 0.2617883190590217\n",
      "Epoch: 11900, Loss: 0.26155373656800857\n",
      "Epoch: 12000, Loss: 0.26131995780683426\n",
      "Epoch: 12100, Loss: 0.2611536123154342\n",
      "Epoch: 12200, Loss: 0.26098723829470644\n",
      "Epoch: 12300, Loss: 0.2608403454599049\n",
      "Epoch: 12400, Loss: 0.2607000319558223\n",
      "Epoch: 12500, Loss: 0.26054553173281814\n",
      "Epoch: 12600, Loss: 0.26036155632508057\n",
      "Epoch: 12700, Loss: 0.25999339148086825\n",
      "Epoch: 12800, Loss: 0.2595887710310731\n",
      "Epoch: 12900, Loss: 0.25929653007941905\n",
      "Epoch: 13000, Loss: 0.2590825964144306\n",
      "Epoch: 13100, Loss: 0.25896049890006995\n",
      "Epoch: 13200, Loss: 0.25879613195761325\n",
      "Epoch: 13300, Loss: 0.25862306323673406\n",
      "Epoch: 13400, Loss: 0.2584787818770837\n",
      "Epoch: 13500, Loss: 0.2583573160278985\n",
      "Epoch: 13600, Loss: 0.2582385099888957\n",
      "Epoch: 13700, Loss: 0.25814976219234326\n",
      "Epoch: 13800, Loss: 0.2580703410984121\n",
      "Epoch: 13900, Loss: 0.2580119434327757\n",
      "Epoch: 14000, Loss: 0.2579627735929576\n",
      "Epoch: 14100, Loss: 0.2579193828159008\n",
      "Epoch: 14200, Loss: 0.25788154394451035\n",
      "Epoch: 14300, Loss: 0.25784014437658054\n",
      "Epoch: 14400, Loss: 0.2578000927765357\n",
      "Epoch: 14500, Loss: 0.257762838411136\n",
      "Epoch: 14600, Loss: 0.2577259948945543\n",
      "Epoch: 14700, Loss: 0.25768929886898756\n",
      "Epoch: 14800, Loss: 0.2576558318248436\n",
      "Epoch: 14900, Loss: 0.25762636921756005\n",
      "Epoch: 15000, Loss: 0.25759827368108834\n",
      "Epoch: 15100, Loss: 0.2575713748541772\n",
      "Epoch: 15200, Loss: 0.2575417528110592\n",
      "Epoch: 15300, Loss: 0.2575124781663086\n",
      "Epoch: 15400, Loss: 0.2574883731223383\n",
      "Epoch: 15500, Loss: 0.2574657149597594\n",
      "Epoch: 15600, Loss: 0.2574446369232855\n",
      "Epoch: 15700, Loss: 0.25741961157620746\n",
      "Epoch: 15800, Loss: 0.25739001598072236\n",
      "Epoch: 15900, Loss: 0.2573549598493019\n",
      "Epoch: 16000, Loss: 0.2573247432024495\n",
      "Epoch: 16100, Loss: 0.2572919081226142\n",
      "Epoch: 16200, Loss: 0.2572652844234186\n",
      "Epoch: 16300, Loss: 0.2572384756336327\n",
      "Epoch: 16400, Loss: 0.25721214261071346\n",
      "Epoch: 16500, Loss: 0.2571871294439601\n",
      "Epoch: 16600, Loss: 0.2571609017162557\n",
      "Epoch: 16700, Loss: 0.25713775234063296\n",
      "Epoch: 16800, Loss: 0.257114405559914\n",
      "Epoch: 16900, Loss: 0.25709116761626616\n",
      "Epoch: 17000, Loss: 0.2570686639554386\n",
      "Epoch: 17100, Loss: 0.2570394598550719\n",
      "Epoch: 17200, Loss: 0.257014813047499\n",
      "Epoch: 17300, Loss: 0.25699071738589485\n",
      "Epoch: 17400, Loss: 0.25696791935116314\n",
      "Epoch: 17500, Loss: 0.25694378321195566\n",
      "Epoch: 17600, Loss: 0.25692150418046233\n",
      "Epoch: 17700, Loss: 0.25689926457104445\n",
      "Epoch: 17800, Loss: 0.25687352825313725\n",
      "Epoch: 17900, Loss: 0.256853216518642\n",
      "Epoch: 18000, Loss: 0.25683597754872745\n",
      "Epoch: 18100, Loss: 0.2568201157656293\n",
      "Epoch: 18200, Loss: 0.25680518658833834\n",
      "Epoch: 18300, Loss: 0.2567921269071511\n",
      "Epoch: 18400, Loss: 0.2567796505275518\n",
      "Epoch: 18500, Loss: 0.2567669643400378\n",
      "Epoch: 18600, Loss: 0.2567537223870654\n",
      "Epoch: 18700, Loss: 0.2567420706396237\n",
      "Epoch: 18800, Loss: 0.2567317387481423\n",
      "Epoch: 18900, Loss: 0.2567221965689763\n",
      "Epoch: 19000, Loss: 0.25671304870073636\n",
      "Epoch: 19100, Loss: 0.2567044902353856\n",
      "Epoch: 19200, Loss: 0.25669547846101715\n",
      "Epoch: 19300, Loss: 0.25668700974926967\n",
      "Epoch: 19400, Loss: 0.2566764556003399\n",
      "Epoch: 19500, Loss: 0.25666856083640605\n",
      "Epoch: 19600, Loss: 0.2566601633213558\n",
      "Epoch: 19700, Loss: 0.25665250607030776\n",
      "Epoch: 19800, Loss: 0.25664485326142455\n",
      "Epoch: 19900, Loss: 0.2566382616263581\n"
     ]
    }
   ],
   "source": [
    "## Training\n",
    "n = len(y_train)\n",
    "epochs = 20000\n",
    "learning_rate = 0.03\n",
    "\n",
    "## Weights\n",
    "w1 = np.random.randn(9, 4) * 0.01\n",
    "b1 = np.zeros((1, 4)) * 0.01\n",
    "\n",
    "w2 = np.random.randn(4, 4) * 0.01\n",
    "b2 = np.zeros((1, 4)) * 0.01\n",
    "\n",
    "w3 = np.random.randn(4, 1) * 0.01\n",
    "b3 = np.zeros((1, 1)) * 0.01\n",
    "\n",
    "for i in range(epochs):\n",
    "    ## Forward Prop\n",
    "    h1 = relu(np.dot(X_train_norm, w1) + b1)\n",
    "    h2 = relu(np.dot(h1, w2) + b2)\n",
    "    y_train_hat_norm = np.dot(h2, w3) + b3\n",
    "\n",
    "    ## Back Propogation\n",
    "    loss = np.mean((y_train_norm - y_train_hat_norm)**2)\n",
    "\n",
    "    dl_dy_hat = (2/n) * (y_train_hat_norm - y_train_norm)\n",
    "    dl_w3 = np.dot(h2.T, dl_dy_hat)\n",
    "    dl_b3 = np.sum(dl_dy_hat, axis=0, keepdims=True)\n",
    "\n",
    "    dl_h2 = np.dot(dl_dy_hat, w3.T) * relu_derivative(h2)\n",
    "    dl_w2 = np.dot(h1.T, dl_h2)\n",
    "    dl_b2 = np.sum(dl_h2, axis=0, keepdims=True)\n",
    "\n",
    "    dl_h1 = np.dot(dl_h2, w2.T) * relu_derivative(h1)\n",
    "    dl_w1 = np.dot(X_train_norm.T, dl_h1)\n",
    "    dl_b1 = np.sum(dl_h1, axis=0, keepdims=True)\n",
    "\n",
    "    ## Subtracting weights\n",
    "    w3 -= learning_rate * dl_w3\n",
    "    b3 -= learning_rate * dl_b3\n",
    "\n",
    "    w2 -= learning_rate * dl_w2\n",
    "    b2 -= learning_rate * dl_b2\n",
    "\n",
    "    w1 -= learning_rate * dl_w1\n",
    "    b1 -= learning_rate * dl_b1\n",
    "\n",
    "    if (i % 100 == 0):\n",
    "        print(f\"Epoch: {i}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 408282.31, Actual: 500001.0, Difference: 91718.69\n",
      "Predicted: 271564.30, Actual: 450000.0, Difference: 178435.70\n",
      "Predicted: 330649.86, Actual: 332400.0, Difference: 1750.14\n",
      "Predicted: 380103.86, Actual: 394400.0, Difference: 14296.14\n",
      "Predicted: 276303.06, Actual: 311900.0, Difference: 35596.94\n",
      "Predicted: 234743.91, Actual: 376000.0, Difference: 141256.09\n",
      "Predicted: 232225.96, Actual: 360300.0, Difference: 128074.04\n",
      "Predicted: 298652.87, Actual: 410000.0, Difference: 111347.13\n",
      "Predicted: 323085.40, Actual: 500000.0, Difference: 176914.60\n",
      "Predicted: 310744.29, Actual: 500001.0, Difference: 189256.71\n",
      "Predicted: 301304.65, Actual: 500001.0, Difference: 198696.35\n",
      "Predicted: 297095.36, Actual: 405200.0, Difference: 108104.64\n",
      "Predicted: 486128.15, Actual: 500001.0, Difference: 13872.85\n",
      "Predicted: 205344.43, Actual: 488900.0, Difference: 283555.57\n",
      "Predicted: 272951.91, Actual: 381300.0, Difference: 108348.09\n",
      "Predicted: 270517.89, Actual: 475000.0, Difference: 204482.11\n",
      "Predicted: 284425.37, Actual: 500001.0, Difference: 215575.63\n",
      "Predicted: 330493.57, Actual: 422200.0, Difference: 91706.43\n",
      "Predicted: 433031.71, Actual: 406300.0, Difference: 26731.71\n",
      "Predicted: 393604.00, Actual: 392300.0, Difference: 1304.00\n",
      "Predicted: 486128.15, Actual: 500001.0, Difference: 13872.85\n",
      "Predicted: 447776.89, Actual: 500001.0, Difference: 52224.11\n",
      "Predicted: 486128.15, Actual: 500001.0, Difference: 13872.85\n",
      "Predicted: 350384.70, Actual: 500001.0, Difference: 149616.30\n",
      "Predicted: 67111.80, Actual: 205000.0, Difference: 137888.20\n",
      "Predicted: 271704.94, Actual: 274400.0, Difference: 2695.06\n",
      "Predicted: 421692.66, Actual: 363600.0, Difference: 58092.66\n",
      "Predicted: 349775.38, Actual: 316700.0, Difference: 33075.38\n",
      "Predicted: 355481.95, Actual: 326000.0, Difference: 29481.95\n",
      "Predicted: 298749.02, Actual: 260700.0, Difference: 38049.02\n",
      "Predicted: 255084.36, Actual: 277800.0, Difference: 22715.64\n",
      "Predicted: 317403.16, Actual: 277500.0, Difference: 39903.16\n",
      "Predicted: 407866.04, Actual: 445700.0, Difference: 37833.96\n",
      "Predicted: 409825.23, Actual: 474000.0, Difference: 64174.77\n",
      "Predicted: 478790.63, Actual: 500001.0, Difference: 21210.37\n",
      "Predicted: 486128.15, Actual: 500001.0, Difference: 13872.85\n",
      "Predicted: 449181.00, Actual: 500001.0, Difference: 50820.00\n",
      "Predicted: 457987.79, Actual: 500001.0, Difference: 42013.21\n",
      "Predicted: 384557.19, Actual: 500001.0, Difference: 115443.81\n",
      "Predicted: 486128.15, Actual: 500001.0, Difference: 13872.85\n",
      "Predicted: 306712.21, Actual: 500001.0, Difference: 193288.79\n",
      "Predicted: 414891.02, Actual: 500001.0, Difference: 85109.98\n",
      "Predicted: 349787.35, Actual: 500001.0, Difference: 150213.65\n",
      "Predicted: 376245.48, Actual: 388500.0, Difference: 12254.52\n",
      "Predicted: 486128.15, Actual: 500001.0, Difference: 13872.85\n",
      "Predicted: 311989.19, Actual: 272400.0, Difference: 39589.19\n",
      "Predicted: 238909.06, Actual: 267400.0, Difference: 28490.94\n",
      "Predicted: 296954.16, Actual: 295300.0, Difference: 1654.16\n",
      "Predicted: 188935.59, Actual: 213500.0, Difference: 24564.41\n",
      "Predicted: 264788.09, Actual: 417500.0, Difference: 152711.91\n",
      "Predicted: 188551.85, Actual: 204200.0, Difference: 15648.15\n",
      "Predicted: 313282.46, Actual: 188300.0, Difference: 124982.46\n",
      "Predicted: 210221.41, Actual: 180100.0, Difference: 30121.41\n",
      "Predicted: 294258.89, Actual: 209800.0, Difference: 84458.89\n",
      "Predicted: 193283.25, Actual: 100000.0, Difference: 93283.25\n",
      "Predicted: 322136.66, Actual: 196800.0, Difference: 125336.66\n",
      "Predicted: 160589.98, Actual: 148600.0, Difference: 11989.98\n",
      "Predicted: 152980.34, Actual: 171000.0, Difference: 18019.66\n",
      "Predicted: 367083.37, Actual: 280800.0, Difference: 86283.37\n",
      "Predicted: 225410.95, Actual: 176600.0, Difference: 48810.95\n",
      "Predicted: 174899.56, Actual: 156100.0, Difference: 18799.56\n",
      "Predicted: 191772.90, Actual: 267000.0, Difference: 75227.10\n",
      "Predicted: 223298.16, Actual: 254400.0, Difference: 31101.84\n",
      "Predicted: 261121.70, Actual: 169300.0, Difference: 91821.70\n",
      "Predicted: 226724.19, Actual: 174800.0, Difference: 51924.19\n",
      "Predicted: 243637.57, Actual: 214600.0, Difference: 29037.57\n",
      "Predicted: 208926.16, Actual: 183200.0, Difference: 25726.16\n",
      "Predicted: 201071.21, Actual: 176100.0, Difference: 24971.21\n",
      "Predicted: 200603.82, Actual: 189100.0, Difference: 11503.82\n",
      "Predicted: 177128.96, Actual: 114600.0, Difference: 62528.96\n",
      "Predicted: 180896.06, Actual: 196300.0, Difference: 15403.94\n",
      "Predicted: 197989.13, Actual: 151200.0, Difference: 46789.13\n",
      "Predicted: 194791.82, Actual: 154700.0, Difference: 40091.82\n",
      "Predicted: 186592.30, Actual: 160300.0, Difference: 26292.30\n",
      "Predicted: 190214.24, Actual: 162100.0, Difference: 28114.24\n",
      "Predicted: 187663.95, Actual: 175200.0, Difference: 12463.95\n",
      "Predicted: 176062.10, Actual: 92500.0, Difference: 83562.10\n",
      "Predicted: 256812.61, Actual: 138200.0, Difference: 118612.61\n",
      "Predicted: 194848.00, Actual: 169600.0, Difference: 25248.00\n",
      "Predicted: 180658.75, Actual: 163400.0, Difference: 17258.75\n",
      "Predicted: 180955.76, Actual: 106300.0, Difference: 74655.76\n",
      "Predicted: 194821.04, Actual: 132500.0, Difference: 62321.04\n",
      "Predicted: 214125.51, Actual: 184700.0, Difference: 29425.51\n",
      "Predicted: 218104.30, Actual: 191100.0, Difference: 27004.30\n",
      "Predicted: 241349.82, Actual: 192300.0, Difference: 49049.82\n",
      "Predicted: 207013.17, Actual: 180400.0, Difference: 26613.17\n",
      "Predicted: 206850.46, Actual: 195500.0, Difference: 11350.46\n",
      "Predicted: 202946.17, Actual: 171200.0, Difference: 31746.17\n",
      "Predicted: 197228.23, Actual: 166700.0, Difference: 30528.23\n",
      "Predicted: 199005.44, Actual: 171100.0, Difference: 27905.44\n",
      "Predicted: 219367.35, Actual: 218900.0, Difference: 467.35\n",
      "Predicted: 226745.33, Actual: 185300.0, Difference: 41445.33\n",
      "Predicted: 264351.87, Actual: 174400.0, Difference: 89951.87\n",
      "Predicted: 204340.33, Actual: 168300.0, Difference: 36040.33\n",
      "Predicted: 207423.35, Actual: 182900.0, Difference: 24523.35\n",
      "Predicted: 205492.49, Actual: 168300.0, Difference: 37192.49\n",
      "Predicted: 198955.39, Actual: 210700.0, Difference: 11744.61\n",
      "Predicted: 360077.06, Actual: 406200.0, Difference: 46122.94\n",
      "Predicted: 323137.01, Actual: 500001.0, Difference: 176863.99\n",
      "Predicted: 380778.00, Actual: 500001.0, Difference: 119223.00\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "\n",
    "X_test_norm = (X_test - X_mean) / X_std\n",
    "\n",
    "h1 = relu(np.dot(X_test_norm, w1) + b1)\n",
    "h2 = relu(np.dot(h1, w2) + b2)\n",
    "y_test_hat_norm = np.dot(h2, w3) + b3\n",
    "\n",
    "y_test_hat = (y_test_hat_norm * y_std) + y_mean\n",
    "loss = np.abs(y_test - y_test_hat)\n",
    "\n",
    "for i in range(100):\n",
    "    print(f'Predicted: {y_test_hat[i][0]:.2f}, Actual: {y_test[i][0]}, Difference: {loss[i][0]:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
